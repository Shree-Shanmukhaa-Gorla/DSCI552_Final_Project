{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/shree/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, initializers, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000    # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 100 # max length of each entry (sentence), including padding\n",
    "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
    "EMBEDDING_DIM = 100      # embedding dimensions for word vectors (word2vec/GloVe)\n",
    "GLOVE_DIR = \"./glove.6B/glove.6B.\"+str(EMBEDDING_DIM)+\"d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Presence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Help me!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Help me!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Front desk clerk Shawna Vela said she dialed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've been shot,' \" said Rosalinda Gonzalez, an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mid-Market and the Tenderloin are home to a th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20181</th>\n",
       "      <td>I'd flip open a cell phone, turn on its camera...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20182</th>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20183</th>\n",
       "      <td>)The biggest challenge in documenting my dinin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20184</th>\n",
       "      <td>Ahumdinger TV season wrapped Wednesday night, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20185</th>\n",
       "      <td>\"The O.C.,\" which also enjoyed a post-\"Idol\" r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20186 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  Presence\n",
       "0                                               Help me!         1\n",
       "1                                               Help me!         1\n",
       "2      \"Front desk clerk Shawna Vela said she dialed ...         0\n",
       "3      I've been shot,' \" said Rosalinda Gonzalez, an...         0\n",
       "4      Mid-Market and the Tenderloin are home to a th...         1\n",
       "...                                                  ...       ...\n",
       "20181  I'd flip open a cell phone, turn on its camera...         1\n",
       "20182                                                  .         0\n",
       "20183  )The biggest challenge in documenting my dinin...         0\n",
       "20184  Ahumdinger TV season wrapped Wednesday night, ...         1\n",
       "20185  \"The O.C.,\" which also enjoyed a post-\"Idol\" r...         0\n",
       "\n",
       "[20186 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('content_baseline.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Sentence']\n",
    "y = df['Presence'].values\n",
    "\n",
    "X = list(X)\n",
    "# for line in tqdm_notebook(X):\n",
    "#     print(clean_text(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    output = \"\"\n",
    "    text = str(text).replace(\"\\n\", \"\")\n",
    "    text = re.sub(r'[^\\w\\s]','',text).lower()\n",
    "    if remove_stopwords:\n",
    "        text = text.split(\" \")\n",
    "        for word in text:\n",
    "            if word not in stopwords.words(\"english\"):\n",
    "                output = output + \" \" + word\n",
    "    else:\n",
    "        output = text\n",
    "    return str(output.strip())[1:-3].replace(\"  \", \" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shree/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2348f7ee60114dcf9bc0d0de71b96000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20186.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [] \n",
    "\n",
    "for line in tqdm_notebook(X): \n",
    "    texts.append(clean_text(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data: ortheast corner market pretty quiet farther block seventh sixth theres another larger group young gentlemen usua 1\n"
     ]
    }
   ],
   "source": [
    "print('Sample data:', texts[5], y[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 49540\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Vocabulary size:', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (20186, 100)\n",
      "Shape of label tensor: (20186,)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = int(VALIDATION_SPLIT*data.shape[0])\n",
    "x_train = data[: -num_validation_samples]\n",
    "y_train = labels[: -num_validation_samples]\n",
    "x_val = data[-num_validation_samples: ]\n",
    "y_val = labels[-num_validation_samples: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of entries in each category:')\n",
    "# print('training: ', y_train.sum(axis=0))\n",
    "# print('validation: ', y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences: \n",
      " [1542 2219 2544 2255    7    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "One hot label: \n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "print('Tokenized sentences: \\n', data[5])\n",
    "print('One hot label: \\n', labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe from: ./glove.6B/glove.6B.100d.txt ...Done.\n",
      " Proceeding with Embedding Matrix... Completed!\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR)\n",
    "print('Loading GloVe from:', GLOVE_DIR,'...', end='')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "f.close()\n",
    "print(\"Done.\\n Proceeding with Embedding Matrix...\", end=\"\")\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(\" Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                           EMBEDDING_DIM,\n",
    "                           weights = [embedding_matrix],\n",
    "                           input_length = MAX_SEQUENCE_LENGTH,\n",
    "                           trainable=False,\n",
    "                           name = 'embeddings')\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.Bidirectional(\n",
    "#     layer, merge_mode=\"concat\", weights=None, backward_layer=None, **kwargs\n",
    "# )\n",
    "\n",
    "# fwd_layer = LSTM(100, return_sequences=True, name='lstm_layer')(embedded_sequences)\n",
    "# bkwd_layer = LSTM(100, return_sequences=True, go_backwards=True)\n",
    "\n",
    "# x = Bidirectional(fwd_layer, bkwd_layer)\n",
    "x = LSTM(100, return_sequences=True, name='lstm_layer')(embedded_sequences)\n",
    "# x = GlobalMaxPool1D()(x)\n",
    "# x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "preds = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embeddings (Embedding)       (None, 100, 100)          4954100   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 100, 100)          80400     \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 100, 50)           5050      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 100, 1)            51        \n",
      "=================================================================\n",
      "Total params: 5,039,601\n",
      "Trainable params: 85,501\n",
      "Non-trainable params: 4,954,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress:\n",
      "Epoch 1/10\n",
      "505/505 [==============================] - 13s 23ms/step - loss: 0.6781 - accuracy: 0.5618\n",
      "Epoch 2/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.6149 - accuracy: 0.6470\n",
      "Epoch 3/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.6021 - accuracy: 0.6582\n",
      "Epoch 4/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.5937 - accuracy: 0.6697\n",
      "Epoch 5/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.5805 - accuracy: 0.6798\n",
      "Epoch 6/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.5688 - accuracy: 0.6861\n",
      "Epoch 7/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.5502 - accuracy: 0.7061\n",
      "Epoch 8/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.5176 - accuracy: 0.7307\n",
      "Epoch 9/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.4875 - accuracy: 0.7538\n",
      "Epoch 10/10\n",
      "505/505 [==============================] - 12s 23ms/step - loss: 0.4472 - accuracy: 0.7805\n"
     ]
    }
   ],
   "source": [
    "print('Training progress:')\n",
    "history = model.fit(x_train, y_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.4189127 ],\n",
       "        [0.32357314],\n",
       "        [0.41888142],\n",
       "        ...,\n",
       "        [0.3465091 ],\n",
       "        [0.34562534],\n",
       "        [0.34474844]],\n",
       "\n",
       "       [[0.43036714],\n",
       "        [0.40007567],\n",
       "        [0.5015754 ],\n",
       "        ...,\n",
       "        [0.5400595 ],\n",
       "        [0.53955805],\n",
       "        [0.5390617 ]],\n",
       "\n",
       "       [[0.52836776],\n",
       "        [0.61595356],\n",
       "        [0.76090324],\n",
       "        ...,\n",
       "        [0.9630629 ],\n",
       "        [0.96305895],\n",
       "        [0.963055  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.44722772],\n",
       "        [0.63716537],\n",
       "        [0.5284948 ],\n",
       "        ...,\n",
       "        [0.79211164],\n",
       "        [0.79180765],\n",
       "        [0.79150546]],\n",
       "\n",
       "       [[0.54270905],\n",
       "        [0.5237188 ],\n",
       "        [0.6275068 ],\n",
       "        ...,\n",
       "        [0.9682194 ],\n",
       "        [0.9682057 ],\n",
       "        [0.968192  ]],\n",
       "\n",
       "       [[0.4436888 ],\n",
       "        [0.43925738],\n",
       "        [0.33890706],\n",
       "        ...,\n",
       "        [0.5064143 ],\n",
       "        [0.50569   ],\n",
       "        [0.50497305]]], dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_val)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403700"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list = []      \n",
    "for sublist in pred:\n",
    "    for item in sublist:\n",
    "        pred_list.append(int(item))\n",
    "\n",
    "for i in range(len(pred_list)):\n",
    "    if pred_list[i] < 0.5:\n",
    "        pred_list[i] = 0\n",
    "    else:\n",
    "        pred_list[i] = 1\n",
    "\n",
    "len(pred_list)\n",
    "        \n",
    "# for i in pred:\n",
    "#     for j in i:\n",
    "#         if j < 0.5:\n",
    "#             j = 0\n",
    "#         else:\n",
    "#             j = 1\n",
    "#         pred_list.append(int(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4037, 403700]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-297-1cdaaac69aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4037, 403700]"
     ]
    }
   ],
   "source": [
    "pred_list = np.array(pred_list)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_val, pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal\n",
    "\n",
    "epochs: 50, \n",
    "score:  63.76 \n",
    "    \n",
    "epoch: 40,\n",
    "score: 64.10\n",
    "    \n",
    "epoch: 30\n",
    "score: 65.04\n",
    "    \n",
    "epoch: 20\n",
    "score: 63.76\n",
    "    \n",
    "epoch: 10\n",
    "score: 65.27\n",
    "    \n",
    "===========================================\n",
    "Global MaxPool Layer\n",
    "\n",
    "epochs: 50, \n",
    "score:  63.85\n",
    "    \n",
    "epoch: 40,\n",
    "score: 64.03\n",
    "    \n",
    "epoch: 30\n",
    "score: 63.16\n",
    "    \n",
    "epoch: 20\n",
    "score: 63.83\n",
    "    \n",
    "epoch: 10\n",
    "score: 64.13\n",
    "\n",
    "===========================================\n",
    "Global MaxPool Layer with 2 dropout layers with parameter = 0.1\n",
    "\n",
    "epochs: 50, \n",
    "score: 64.87\n",
    "    \n",
    "epoch: 40,\n",
    "score: 63.78\n",
    "    \n",
    "epoch: 30\n",
    "score: 66.53\n",
    "    \n",
    "epoch: 20\n",
    "score: 64.47\n",
    "    \n",
    "epoch: 10\n",
    "score: 64.80\n",
    "    \n",
    "============================================\n",
    "Global MaxPool Layer with 2 dropout layers with parameter = 0.2\n",
    "\n",
    "epochs: 50, \n",
    "score: 64.55\n",
    "    \n",
    "epoch: 40, \n",
    "score: 64.89\n",
    "    \n",
    "epoch: 30\n",
    "score: 64.20\n",
    "    \n",
    "epoch: 20\n",
    "score: 64.40\n",
    "    \n",
    "epoch: 10\n",
    "score: 65.02\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
